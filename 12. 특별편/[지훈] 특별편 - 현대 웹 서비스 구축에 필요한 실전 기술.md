
# 현대 웹 서비스 구축에 필요한 실전 기술

# 작업큐 시스템

## 웹 서비스와 요청

웹 서비스에서는 기본적으로 요청이 동기적으로 실행된다.
( 즉 요청에 기인하는 모든 처리가 끝난 다음에 응답이 반환된다.)

따라서 계속 성장해가는 웹 서비스에서는 데이터가 서서히 축적되면서
데이터를 추가하고 갱신하는 처리가 점점 무거워진다.

→ 양호했던 성능도 시간이 지남에 따라 악화되고 서비스 사용자 경험에 영향을 주는 경우가 발생

이런 경우에 작업 큐 시스템을 사용함으로써 
나중으로 미뤄도 되는 처리를 비동기로 실행할 수 있고 사용자 경험도 개선할 수 있다.

## 작업큐 시스템 입문

### 가장 간단한 비동기 처리 방법 : 스크립트 실행

가장 간단한 방법은
비동기화 하고자 하는 처리를 독립된 스크립트로 만들고 애플리케이션에서 그걸 호출하는 방법이다.

- 장점 : 간단하다.
- 단점
    - 스크립트 시작과 초기화의 오버헤드가 큼 → 성능이 느려짐
    - 일시적으로 대량의 비동기 처리를 하려면 그만큼 프로세스를 실행 시켜야함 → 성능 느려짐

### 작업큐 시스템

어느 정도 양이 있는 비동기 처리를 안정적으로 수행하려면 
”작업큐와 워커를 세트로 한 작업 큐 시스템을 사용하는 것”이 일반적이다.

작업큐 시스템에서는 

![image.png](attachment:c96ce5ec-d024-4b3e-8993-e07d7f0d305f:image.png)

1. 작업 큐에 실행하고자 하는 처리를 등록하고
2. 워커가 큐에서 작업을 추출해서 실제로 처리한다.

### 효과

- 작업큐를 통해서 일시적으로 대량의 처리가 등록되었을 때 부하의 변동을 흡수할 수가 있다.
- 워커는 항상 실행해둠으로써 작업을 처리할 때 초기화 오버헤드를 거의 없앨 수 있다.

## 하테나에서의 작업큐 시스템

하테나에서는 TheSchwartz와 German을 사용하고 있다.
하지만 위 두 작업큐 시스템은 워커 프로세스를 세세하게 지원하는 기능은 제공하지 않아
자체적으로 개발한 WorderManager를 TheSchwartz와 German의 워커 프로세스를 관리하기 위해 사용하고 있다.

- TheSchwartz와 Gearman을 래핑해서 최소한의 변경으로 양쪽에 대응
- 설정 파일로 워커 클래스 정의. 설정 파일만 수정해서 워커로서 사용할 클래스를 변경할 수 있따
- 워커 프로세스의 라이프사이클 관리. 프로세스 관리, 데몬화 수행
- 워커 프로세스의 프로세스 개수 관리. 프로세스 개수를 관리하고 병행처리가 가능한 작업수를 제어한다
- 로그 출력. 작업을 처리한 타임스탬프 등을 로그에 기록한다.

# 스토리지 선택

## 증가하는 데이터를 어떻게 저장할까?

큰 데이터를 다루는 스토리지는 약간의 구성변경, 엑세스 패턴 변화로 응답속도 저하가 되는 경우가 있다.
따라서 데이터량, 스키마, 액세스 패턴에 맞는 스토리지 선택하는 것이 중요하다.

### 웹 애플리케이션과 스토리지

스토리지란 애플리케이션 데이터를 영속적으로 혹은 일시적으로 저장하기 위한 기능이라고 할 수 있다.

애플리케이션 데이터의 종류

- 본질적으로 없어질 수 없는 원본 데이터
- 원본 데이터를 가공함으로써 생성된 액세스 랭킹이나 검색용 인덱스 데이터 등 재생성 가능한 가공 데이터
- 캐시와 같이 사라져도 성능상의 문제 이외에는 다른 문제가 없는 데이터

각 데이터 별 특성

- 원본 데이터는 가장 중요해서 서비스의 근본적인 신뢰성과 관계되어 있으므로 그에 상응하는 비용을 들여서 최상급 신뢰성을 확보해야 한다.
- 반면, 캐시와 같은 데이터의 신뢰성은 그다지 중요시되지 않고 성능을 높이거나 비용을 줄일 필요가 있다.

### 적절한 스토리지 선택의 어려움

스토리지를 잘못 선택한 채로 서비스를 시작하게 되면 나중에 알아차리더라도 스토리지 변경은 보통 방법으로는 뜻대로 이룰 수 없다.
특히 서비스를 시작한 후에 순조롭게 인기를 얻어가면서 자주 이용하게 된 서비스의 경우,
저장된 데이터량도 커지고 서비스 정지의 영향도 커서 더욱더 어려워진다.

→ 따라서 저장하고자 하는 데이터의 특성에 맞는 스토리지를 선택하는 것이 중요하다!

## 스토리지 선택의 전제가 되는 조건

스토리지를 선택할 때에는 애플리케이션에서의 액세스 패턴을 이해하는 것이 중요하다.
액세스 패턴으로는 아래 6가지 지표가 중요한 판단 포인트가 된다.

- 평균 크기
- 최대 크기
- 신규 추가 빈도
- 갱신 빈도
- 삭제 빈도
- 참조 빈도

## 스토리지의 종류

- RDBMS : MySQL, PostgreSQL 등
- 분산 key-value 스토어 : memcached, TokyoTyrant 등
- 분산 파일 시스템 : MogileFS, GlusterFS, Lustre
- 그 밖의 스토리지 : NFS 꼐열 분산 파일시스템, DRBD, HDFS

## RDBMS

- 테이블 형태로 데이터를 저장하고 대부분은 SQL 언어로 데이터 조작을 수행하는 시스템.
- 다양한 데이터를 저장한다거나 강력한 질의를 할 수 있어서 가장 범용적이 높은 스토리지이다.

### MySQL

MySQL의 아키텍처는

- SQL을 해석해서 실행하는 MySQL 엔진과
- 실제 데이터를 보관하는 스토리지 엔진

으로 분리되어 있다.

주요한 스토리지 엔진으로는

- MyISAM
- InnoDB

두가지가 있다.

**MyISAM**

- MyISAM은 insert 조작을 빠르게 할 수 있고, 시작 및 정지가 빠르며 테이블 이동이나 이름 변경을 파일 시스템 조작으로 직접 할 수 있는 등 DB 운용은 용이하다
- 반면에 DB 프로세스가 비정상 종료하면 테이블이 파손될 가능성이 높다거나 트랜잭션 기능이 없고 update, delete, insert가 테이블 락으로 되어 있어서 갱신이 많은 용도로는 성능적으로 불리하다는 등 몇 가지 단점도 존재한다.

**InnoDB**

- **InnoDB**는 **트랜잭션을 지원**하며, **비정상 종료 시 복구 기능**이 있고, **데이터 갱신이 로우 락으로 되어 있는** 등 장점이 있다.
- 다만 데이터량에 따라서는 시작, 정지가 수 분 정도 걸린다거나 테이블 조작을 모두 DB를 경유해서 수행해야 하는 등의 단점도 존재한다.

## 분산 Key-Value 스토어

key-value 스토어는 key와 value 쌍을 저장하기 위한 심플한 스토리지.

분산 key-value 스토어는 **key-value 스토어에 네트워크를 지원함으로써 다수의 서버로 확장**시키는 기능을 지닌 것.

key-value 스토어는 RDBMS에 비해 기능적으로 부족하지만, 성능이 10~100배 이상이라는 게 특징이다.

### memcached

- 파일 시스템을 사용하지 않고 메모리 상에서 동작하므로 매우 빠르게 동작한다.
- 메모리 상에서 동작하기 때문에 재시작할 때 데이터가 모두 사라져 버린다.
    - 따라서 원본 데이터 저장용으로는 부적합!
      → 이러한 특성을 가장 잘 활용할 수 있는 데이터는 캐시 데이터이다.
    - ex) RDBMS에서 읽어 들인 데이터를 일시적으로 저장해 두고
      또다시 참조할 때는 먼저 memcached를 참조해서 찾지 못한 경우에만
      RDBMS를 참조하는 방법이 있다.
- 용도를 캐시로 한정할 경우,
  서버에는 메모리만 충분히 탑재해두면 되며, CPU나 I/O 성능은 그다지 요구되지 않는다.

## 분산 파일시스템

- 파일시스템의 특성상 어느정도 이상인 크기의 데이터를 저장하는데 적합하다.
- NFS와 같이 그것이 고려되고 있는 구현을 제외하고 작은 데이터가 대량으로 존재하는 용도에는 적합하지 않다.
- 하테나에서는 MogileFS를 사용중이다.

  → But, 요즘은 잘 안쓰임

    - Perl 기반 + 유지 보수 중단 → 유지보수가 어렵고 새 기여도 없음
    - 클라우드 오브젝트 스토리지의 등장
        - Amazon S3, Google Cloud Storage, MinIO 같은 시스템이 기능도 많고 훨씬 안정적임
    - 대체 오픈소스가 많아짐
        - Ceph, MinIO, SeaweedFS, HDFS 등 모던한 분산 파일 시스템이 더 많고 활발하게 개발됨


## 그 밖의 스토리지

### NFS 계열 분산 파일 시스템

- 특정 서버의 파일 시스템을 다른 서버에서 마운트해서 해당 서버의 로컬 파일시스템과 마찬가지로 조작할 수 있도록 하는 기술이다
- 단점
    - 커널 레벨에서 구현돼있는 경우가 많아
      서버 측에 장애가 발생하면 클라이언트의 동작도 덩달아서 정지하는 경우 발생함


### WebDAV 서버

- NFS와 같이 커널 계층에 구현돼있는 경우 약간의 불안정함이 바로 장애로 이어지는 경우가 있다.

  → 이때 HTTP 기반인 WebDAV프로토콜을 지원하는 스토리지를 사용할 수 있다.

- 장점
    - 애플리케이션 계층에서 구현되는 경우가 많아서 보다 안정된 시스템을 구축할 수 있다.
- 단점
    - 일반적으로는 마운트 할 수 없으므로 파일 조작을 위해서는 애플리케이션을 수정해야 한다.

### DRBD

- 네트워크 계층의 RAID
- 블록 디바이스 레벨에서 분산, 다중화 할 수 있는 기술로 2대의 스토리지 서버의 블록 디바이스간 동기를 실현한다.

### HDFS

- Hadoop 용 분산 파일 시스템
- 파일을 64MB씩 분할해서 저장하고,
  수백MB~ 수백GB의 거대한 데이터를 저장하는 것을 목적으로 하고 있다.
- 쉽게 말하면

    <aside>
    💡

  “엄청 큰 파일들을 여러 대의 컴퓨터에 나눠서 안전하게 저장하고,
  동시에 빠르게 읽고 쓸 수 있게 도와주는 시스템”

    </aside>


## 스토리지의 선택전략

![image.png](attachment:5d33bc64-196f-4fdd-9a56-a3332096c344:image.png)

# 캐시 시스템

## **웹 애플리케이션의 부하와 프록시/캐시 시스템**

웹 애플리케이션의 부하가 증가해서 시스템 용량이 부족해졌을 때는 AP 서버나 DB 서버를 증설함으로써 대응할 수도 있지만, 
HTTP 레벨의 캐싱을 수행하는 HTTP 가속기를 사용함으로써 낮은 비용으로 효과가 높은 대책을 세울 수 있다.

### HTTP 가속기 종류

![image.png](attachment:90a37b10-6f64-4e97-aedb-1613aa50e81f:image.png)

- **포워드 프록시**
    - 클라이언트가 외부 서버에 액세스 할 때 사이에 두는 프록시
- **리버스 프록시**
    - 외부의 클라이언트가 내부 서버에 액세스 할 때 사이에 두는 프록시

### 효과

프록시에서는 요청에 대한 응답을 캐싱해둠으로써 다음에 같은 요청이 전달됐을 때 캐싱해둔 응답을 반환할 수 있다. → 이에 따라 대역이나 서버 리소스를 소비하지 않고 빠르게 요청을 처리할 수 있다.

### 리버스 프록시 캐시 서버

리버스 프록시 캐시 서버의 구현으로 Squid와 Varnish가 가장 유명하다.

→ 🙅‍♂️ No!  Nginx/CloudFront/Cloudflare 등으로 많이 대체됨

## **기본적인 구성**

![image.png](attachment:18807fb6-643d-4252-bce5-4d3f41bb5104:image.png)

리버시 프록시와 AP 서버 2대로 이루어진 구성에서 캐시 서버를 도입할 경우,
리버시 프록시와 AP 서버 사이에 배치한다.

이에 따라 리버스 프록시에서 AP 서버로 전송되고 있던 요청 중 일부를 캐시 서버에서 처리할 수 있게 되어 시스템 전체 성능을 향상할 수가 있다.

### 효과

- AP 서버로 전송되는 요청수를 줄일 수 있으므로 AP 서버의 대수 증가를 억제, 감소할 수 있다.
- 액세스 집중 시에는 방대한 요청으로 인해 시스템 전체의 수용능력을 넘어서는 것을 막는 효과를 기대할 수 있다. 이를 위해 액세스가 집중된 콘텐츠를 캐시 서버에서 캐싱한다.

### **2단 구성 캐시 서버**

![image.png](attachment:481b84d6-e9ac-47cc-bf64-f052f471ede5:image.png)

- 이미지 파일 등 크기가 큰 파일을 캐싱하게 되면서 캐시 서버의 부하가 높아지면
  1대나 2대 정도로는 용량이 턱없이 부족해질 경우가 있다.
- 이런 경우 캐싱 서버를 2단으로 구성함으로써 보다 확장성이 높은 캐시 서버군을 구성할 수 있다.

### CARP

- 특정 URL을 항상 같은 캐시 서버로 라우팅해서 캐시 효율을 높이기 위한 분산 해싱 프로토콜
- 분산 전략
    - 각 서버마다 고유 ID (IP 등)를 갖고
    - 요청 키와 서버 ID로 해시 점수 계산 → 점수가 가장 높은 서버로 요청 전달

      > 예:
      >
      >
      > `score = hash(URL + server_IP)`
      >
      > → 점수 가장 높은 서버 선택
    >
    - ❗**서버 수가 바뀌면 전체 해시 점수 다시 계산해야 해서 많은 키가 이동**

→ Squid를 사용하는 경우가 아니면 사용X

❗왜?

- CARP는 외부 프록시가 여러 독립 캐시 서버에 요청 분산을 강제하는 방식인데,
  요즘은 캐시 서버 클러스터나 프록시가 분산 로직을 자체 내장하거나,
  Consistent Hashing 등 더 효율적인 해시 분산을 적용해 CARP 같은 특수 프로토콜이 불필요해졌다.

### 요즘 많이 사용하는 구조 ( 리버스 프록시 + 분산 캐시 서버 + Consistent Hashing 기반 )

![image.png](attachment:9618d307-dc41-4bdf-b5ce-0237282298f5:image.png)

Consistent Hashing 방식

- 해시 공간을 원형(360도 링)처럼 구성
- 키도, 서버도 같은 해시 함수로 링 위에 배치
- 키는 **자신보다 가장 가까운 시계방향 서버에 매핑**
    - 서버가 하나 추가되어도, 해당 서버 구간에 있는 일부 키만 이동 → 캐시 무효화 최소화!

[Consistent hashing (Hash ring)](https://binux.tistory.com/119)

### **COSS 크기 결정방법**

히트율을 높이기 위해서는 충분한 캐시 용량을 준비해둘 필요가 있다.
다만 캐시 용량은 크면 클수록 좋은 것이 아니고 과부족이 없는 상태가 최적이다.

만약 캐시 용량이 너무 크면

- 초기 시작 시에 COSS 파일 생성에 시간이 걸린다.
- 서버 재시작 등으로 메모리가 초기화된 후에 디스크 상의 파일이 메모리에 올라가고 캐시 서버의 성능이 안정되기까지 시간이 걸린다.
- 디스크 용량을 압박한다.

반대로 캐시 용량이 너무 적으면

- 필요한 오브젝트가 저장되지 못해서 **캐시 히트율이 떨어진다.**

최적의 용량은

- **1초당 저장된 오브젝트 * 오브젝트의 평균 크기 * 오브젝트의 평균 유효시간로** 계산되는 크기가 된다.

### **주의사항**

캐시 서버의 효율을 올릴수록 필연적으로 캐시 서버에 장애가 발생했을 때의 영향이 커진다.

- 그래서 1대가 고장 나더라도 문제가 없을 정도의 서버를 준비하는 것이 정석이다.

또한 수리된 서버나 새로운 서버를 로드밸런서에 추가할 때 성능이 단번에 떨어져 버리는 경우가 있다.

- 재시작되거나 새로 구성한 캐시 서버가 요청을 처리하기 위한 충분한 준비가 되어 있지 않기 때문.
- 이상적으로는 사전에 평상시에 접수되는 요청을 보내서 워밍업을 해둘 필요가 있다.
- 혹은 로드밸런서에 추가할 때 트래픽 비율을 조정해서 통상 운용 시에 비해 적은 트래픽부터 흘려보내기 시작하는 게 좋다.

# 계산 클러스터

## 대량 로그 데이터의 병렬 처리

대규모 웹 서비스를 운영하면 대용량 로그 데이터가 쌓인다.
대량으로 쌓인 로그 데이터의 처리는 이를 한번에 읽어들이는 것만도 어렵고,
더욱이 통계처리나 분석을 빠르게 하기 위해서는 병렬처리가 가능한 계산 클러스터가 필요하다.

## MapReduce의 계산모델

하테나에서는 Hadoop이라는 MapReduce의 오픈소스 구현을 사용한다.

- MapReduce는 거대한 데이터를 빠르게 병렬로 처리하는 것을 목적으로 하며 이 계산 시스템은 다수의 계산 노드로 구성된 클러스터와 대량 데이터를 분산해서 저장하기 위한 분산 파일 시스템으로 구성된다

### 계산 모델

- MapReduce 계산모델은 key와 value 쌍의 리스트를 입력 데이터로 해서 최종적으로 value의 리스트를 출력한다.
- 계산은 기본적으로 Map단계와 Reduce 단계로 구성된다.

![image.png](attachment:3a075889-721e-4ea6-aa10-87637dd018d1:image.png)

**Map 단계**

- 마스터 노드에서 입력 데이터를 잘게 분할 해서 각 노드로 분산한다
- 각 노드에서는 분할된 입력 데이터를 계산하고 계산 결과를 key-value 쌍으로 구성된 중간데이터로 출력한다.

    ```
    (k1, v1) -> list (k2, v2)
    ```


**Reduce 단계**

- Map단계에서의 출력 데이터를 key(k2)별로 정리해서
  key(k2)와 key 대응하는 값의 리스트(list(v2))로 재구성한다
- 다음으로 각각의 key를 각 노드로 분산한다 ( 이걸 `Shuffle Pahse`라고 함)
- 각 노드에 있는 key(k2)와 key에 대응하는 값의 리스트(list(v2))를 입력 데이터로 해서
  각 리스트(list(v3))를 최종적인 출력데이터로 하는 처리를 수행한다.

    ```
    (k2, list(v2)) -> (k2, list(v3))
    ```

- 최종적으로 각 노드에서 값의 리스트(list(v3))을 집약하면 계산이 완료된다

### 특징

- MapReduce 계산 모델에서는 Map과 Reduce라는 두 가지 처리를 수행하는 함수를 준비하는 것만으로
  대량의 데이터를 빠르게 처리할 수 있다(로그 분석, 검색엔진의 인덱스 생성도 가능)
- MapReduce 계산모델 실행에서는 대량의 입력 데이터를 읽어들이는 부분이 병목이 될 수 있기 때문에
  분산 파일 시스템과 병용하는 것이 좋다.
  (다수의 노드에 사전에 데이터를 분산 배치, 실제로 처리를 실행할 때 가능한 한 데이터가 로컬에 존재하는 노드에서 처리를 실행하도록 한다)