대규모 데이터에 입각해서 OS 캐시에 대해 설명한다.
OS가 캐시를 통해 대규모 데이터를 효율적으로 처리하려한다는 내용이다.

대규모 데이터를 처리하려고 하면 I/O Bound, CPU Bound로 인해 부하가 발생한다.
이 부하를 무조건 분산하기보다는 단일 호스트의 성능을 최대한 최적화 해 본 다음
단일 호스트로는 도저히 안될때  부하 분산을 시도한다.

여기서 OS 캐시로 제대로 처리할 수 없게 되었을때 분산에 대해 고려해보게 되는 것이다.

# 08. OS의 캐시 구조

OS 캐시란 무엇인지 그 원리를 설명한다.

## OS의 캐시 구조를 알고 애플리케이션 작성하기

OS에는 디스크 내의 데이터에 빠르게 엑세스 할 수 있도록 하는 구조가 갖춰져 있다.

OS는 메모리를 이용해서 디스크 I/O를 줄이고 있는데
그 원리가 바로 `OS캐시` 이다.

OS 캐시를 잘 알고 이를 전제로 애플리케이션을 작성하면 OS에 상당 부분을 맡길 수 있다.

( Linux의 경우 `페이지 캐시` 라고 한다. )

Linux의 `페이지 캐시` 는 파일 캐시, 버퍼 캐시라고도 불리지만
파일 캐시라고 하는 것은 적절하지 못한 표현이다.

### 페이지란 무엇인가?

Linux의 페이징 구조를 예로 설명한다.

![Image](https://github.com/user-attachments/assets/d1e257eb-e292-4022-8a68-d70835dd3c7f)

OS는 `가상 메모리 구조` 를 갖추고 있다.

가상 메모리 구조는 논리적인 선형 어드레스를 물리 어드레스로 변환하는 것이다.

### 가상 메모리 구조

가상 메모리 구조가 존재하는 가장 큰 이유는 물리적인 하드웨어를 OS에서 추상화하기 위해서다.

![Image](https://github.com/user-attachments/assets/4773d62c-7d4c-4efc-a73a-a58299ade465)

(1)의 어드레스를 직접 프로그램에서 사용하면 여러 곤란한 일이 일어난다.

따라서 프로세스에서 메모리를 필요로 하게 되면 (1)의 어드레스가 아니;라 메모리에서 비어있는 곳을 찾는다.
메모리는 OS에서 관리하고 있으며

[3] 처럼 비어있는 곳을 반환할 때 (1)어드레스가 아니라 다른 어드레스 (2)를 반환한다.

그 이유는 개뱔 프로세스에서는 메모리의 어느 부분을 사용하는지 관여하지 않고,
’반드시 특정 번지부터 시작’ 또는 ‘0x000부터 시작’하는 것으로 정해져 있으면 다루기 쉽기 때문이다.

핵심은 OS라는 것은 메모리를 직접 프로세스로 넘기는 것이 아니라
일단 커널 내에서 메모리를 추사오하 하고 있다는 점이다.
이것이 가상메모리 구조이다.

강의5에서 디스크의 경우에도 OS가 모아서 읽어낸다고 했는데,
메모리를 확보할 때에도 마찬가지 방식으로
메모리 1바이트씩 액세스 하는 것이 아니라 4KB 정도의 블럭으로 확보해서 프로세스에게 넘긴다.
여기서 한개의 블록을 “페이지”라고 한다.


## Linux의 페이지 캐시 원리

OS는 확보한 페이지를 메모리상에 계속 확보해두는 기능을 갖고 있다.

![Image](https://github.com/user-attachments/assets/33d66d8f-b278-42e7-83a6-a9cd27dc5699)

### **프로세스가 디스크로부터 데이터를 읽어내는 과정**

1. OS는 디스크로부터 “4KB 크기의 블록” ( `페이지` )을 읽고 메모리상에 위치시킨다. 
    
    > 프로세스는 디스크에 직접 액세스 할 수 없다.
   프로세스가 액세스 할 수 있는 것은 (가상)메모리이다.

2. OS는 그 메모리 주소를 프로세스에 알려준다.
3. 그러면 프로세스가 해당 메모리에 액세스하게 된다

### **데이터 읽기를 마친 후**

데이터 읽기를 마친 프로세스가 더이상 데이터가 필요하지 않게 되었어도
OS는 페이지를 해제하지 않고 남겨둔다.

- 그러면 다음에 다른 프로세스가 같은 디스크에 액세스 할 때는 남겨두었던 페이지를 사용할 수 있으므로 디스크를 읽으러 갈 필요가 없게 된다.

이것이 ”페이지 캐시”다.

- 커널이 한 번 할당한 메모리를 해제하지 않고 계속 남겨두는 것이 페이지 캐시의 기본이다.

### **페이지 캐시의 효과**

Linux에서는 디스크에 데이터를 읽으러 가면 꼭 한 번은 메모리로 가서 데이터가 반드시 캐싱된다.
****이러한 이유로 두 번째 이후의 액세스가 빨라진다.
(Linux 뿐만 아니라 대부분의 현대 OS에서 비슷한 구조를 가진다)

이는 예외인 경우를 제외하고 모든 I/O에 투과적으로 작용한다.

**정리하면**

- OS에서 디스크의 내용을 페이지 단위로 메모리에 읽어 들인다.
    - 프로세스는 직접적인 디스크 주소가 아닌 메모리에 읽어들인 페이지 주소를 통해 엑세스한다.
    - 이러한 구조로 OS에서는 디스크 I/O까지 가지 않고 메모리에서 처리할 수 있도록 한다.
- 작성된 페이지는 파기되지 않고 남는다. → 페이지 캐시
    - 다음에 다른 프로세스가 같은 디스크에 액세스 할 때는 남겨두었던 페이지를 사용할 수 있으므로
      디스크를 읽으러 갈 필요가 없게 된다.

## VFS

디스크의 캐시는 페이지 캐시에 의해 제공되지만,
실제 이 디스크를 조작하는 **디바이스 드라이버와 OS 사이에는 파일 시스템이 끼어 있다.**

![Image](https://github.com/user-attachments/assets/d7deab4f-ee15-462d-8c32-5b38b248952b)

파일 시스템 위에는 VFS (Virtual File System, 가상 파일 시스템)이라는 추상화 레이어가 존재한다.

파일 시스템에는 다양한 함수를 갖추고 있는데, 그 인터페이스를 통일하는 것이 VFS의 역할이다.

### **VFS의 특징**

VFS는 페이지 캐시의 구조를 지니고 있는데,
어떤 파일 시스템을 이용하더라도, 어떤 디스크를 읽어내더라도 반드시 동일한 구조로 캐싱된다.

### **정리**

VFS는 파일 시스템 구현의 추상화와 성능에 관련된 페이지 캐시 부분의 역할을 수행한다.

## 파일캐시 ❌  페이지 캐시 ⭕

앞에서 이야기 “Linux의 `페이지 캐시` 는 파일 캐시, 버퍼 캐시라고도 불리지만 파일 캐시라고 하는 것은 적절하지 못한 표현이다” 라는 부분에 대해서 알아보자.

![Image](https://github.com/user-attachments/assets/1303f8a2-3e70-437a-8790-2e1232309acd)

디스크 상에 4GB 정도의 매우 큰 파일이 있고, 메모리가 2G 밖에 없는데 그중 500MB 정도를 OS가 프로세스에 할당했다고 하자.

### 이때 1.5GB 정도의 여유 메모리가 있는 상황에서 4G 파일을 캐싱할 수 있을까?

**파일 캐시라고 생각하면?**
파일 1개 단위로 캐싱하고 있어서 4GB는 캐싱할 수 없다고 생각할 수 있지만, 실제로는 그러지 않다.

**OS는 읽어낸 블록 단위만으로 캐싱할 수 있는 범위가 정해진다.** 

디스크상에 배치되어 있는 4KB 블록만을 캐싱하므로
특정 파일의 일부분만, 읽어낸 부분만을 캐싱할 수 있다.
이렇게 디스크를 캐싱하는 단위가 페이지다.

### (보충) 어떻게 파일 전체가 아닌 일부분만 캐싱 할 수 있을까?

리눅스는 파일을 i노드 번호라는 번호로 식별하고 어느 위치에서 시작하는 오프셋을 제공하여
두가지 정보를 함께 캐싱한다.

이 두 가지를 키로 하면 '어떤 파일의 어느 위치를'이라는 쌍으로 캐시의 키를 관리할 수 있으므로 결과적으로 파일 전체가 아닌 파일의 일부를 캐싱해갈 수 있다.

파일이 아무리 크더라도 이 키로부터 해당 페이지를 찾을 때의 데이터 구조는 최적화되어 있다.
OS 내부에서 사용되고 있는 데이터 구조는 Ratix Tree라고 하며,
파일이 아무리 커지더라도 캐시 탐색속도가 떨어지지 않도록 개발된 데이터 구조다.

## 메모리가 비어있으면 캐싱

리눅스는 메모리가 비어있으면 특별한 제한 없이 모두 캐싱한다.

→ 그래서 메모리가 부족해 보일 수도 있지만 사실 메모리가 부족한게 아니라
OS가 메모리가 비어있는 곳에 디스크를 캐싱하고 있을 뿐이다.

**프로세스에서 메모리를 요청했을 때 캐시로 인해 더이상 메모리가 남아있지 않다면?**
오래된 캐시를 버리고 프로세스에 메모리를 확보해준다.

### 실제 사례

`sar -r` 명령어를 통해 이를 확인 할 수 있다.

![Image](https://github.com/user-attachments/assets/93b344d8-bd44-4da1-828c-2a9bd4cdacdb)

메모리를 99%나 사용중이고 700MB 정도를 캐싱에 할당하고 있다.

- kbcached : 캐싱되어 있는 용량
- %memused : 메모리 사용량

이것만 보면 메모리가 부족해 보이지만 실제로는 그렇지 않다.

단지 메모리가 비어있는 곳에 디스크를 캐싱하고 있을 뿐이며
캐시 이외에 메모리가 필요해지면 오래된 캐시부터 파기한다.

## 메모리를 늘려서 I/O 부하 줄이기

메모리를 늘리면 캐시에 사용할 수 있는 용량이 늘어나고,
캐시에 사용할 수 있는 용량이 늘어나면 보다 많은 데이터를 캐싱할 수 있고,
많이 캐싱되면 디스크를 읽는 횟수가 줄어든다.

### 사례

![Image](https://github.com/user-attachments/assets/86f7c5c1-32ca-4b7e-acd1-d1dc189efa59)

%iowait가 대략 20%이다.
이는 프로세스가 작업을 수행할 때 항상 I/O에서 대기를 한다는 신호이다.

**메모리를 늘려봄**

![Image](https://github.com/user-attachments/assets/bfdef035-73d1-4512-96b0-d9ab2048bf8f)

대기가 거의 없어졌다.

즉, 4GB에서는 전부 캐싱 할 수 없었으나
메모리응 8GB로 늘리고 나니 DB상의 파일을 대부분 캐시로 올릴 수 있었다는 것이다.

### 정리

‘메모리를 늘려서 I/O부하를 줄이자’는 것이 데이터가 많아졌을 때 I/O 부하를 줄이는 기본 방침이다.

- 그럼 메모리만 늘리면 다 해결? → X
- 이 부분은 뒤에서 다시 다룸

## 페이지 캐시는 투과적으로 사용

OS 부팅 직후에는 커널이 디스크를 읽지 않았으므로  캐싱된 내용이 없지만,
갑자기 큰 파일을 읽으면 해당 파일이 캐싱이 되기 때문에 갑자기 메모리 사용 용량이 높아진다.

![Image](https://github.com/user-attachments/assets/b9317420-0159-483f-bd55-262e74a83a8c)

# 09. I/O 부하를 줄이는 방법

OS 캐시의 원리에 대한 이해를 바탕으로,

- 캐시를 전제로한 I/O 부하를 줄이는 방법
- 가능한 한 메모리에서 완결시키기 위한 대책과 메모리만으로 처리 할 수 없어서 여러 서버에 분산시켜야 할지에 대한 지침
- 분산시켜야 한다면 어떻게 확장성을 확보 할 것인지

에 대한 이야기를 다룬다.

## 캐시를 전제로 한 I/O 줄이는 방법

캐시에 의한 I/O 경감 효과는 매우 크기 때문에
캐시를 전제로 하는 것이 I/O 부하를 줄이는 방법의 기본이다.

이 기본으로부터 도출할 수 있는 포인트가 두 가지 있다.

### 도출 포인트

- **다루고자 하는 데이터의 크기에 주목하자.**
    - **데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있다.**
    - 또한 대규모 데이터 처리에서 데이터를 압축해서 저장해두면 디스크 내용을 전부 그대로 캐싱해둘 수 있는 경우가 많다.
- **경제적인 비용과의 밸런스를 고려하자.**
    - 메모리의 경우 비용이 비싸기 때문에
      소프트웨어적으로 메모리 사용을 줄일 수 있다면 최대한 그렇게 하자

## 복수 서버로 확장시키기

### **메모리를 늘려서 전부 캐싱할 수 없는 규모인 경우 어떻게 해야 할까?**

**→ 복수 서버로 확장시키는 방안**을 생각해볼 수 있다.

**AP 서버를 늘리는 방안**

CPU 부하를 분산시키기 위해서

**DB 서버를 늘리는 방안**

반드시 부하 때문만은 아니고
캐시 용량을 늘리고자 할 때 혹은 효율을 높이고자 할 때인 경우가 많다.

DB서버를 늘리면서 I/O 부하를 분산할 수 있지만

I/O 분산에는 국소성을 고려해야하며 마냥 늘려서 좋은 것은 아니다.

### 단순히 대수만 늘려서는 확장성을 확보할 수 없다.

부족한 캐시 용량을 확보하기 위해 단순히 DB 서버를 늘리는 것만으로는 안된다.

![Image](https://github.com/user-attachments/assets/2fead0dd-ccb2-46fc-84e6-808b3a692e3a)

애초에 캐시 용량이 부족해서 서버를 늘렸는데 부족한 캐싱 용량의 상황까지 그대로 복제될 수 있다.
( = 캐싱할 수 없는 비율은 변함없이 그대로… )

- A 서버에서 조회하며 캐싱했는데 부족한 것이 B 서버에서도 동일하게 일어남
- 어느정도 빨라질 수는 있겠지만 증설비용대비 성능향상은 좋지 않다.

# 10. 국소성을 살리는 분산

여러 서버에 분산시킨 경우에도 캐시를 고려하고자 할 때
이를 위해 필요한 국소성 등의 사고방식을 소개한다.

## **국소성을 고려한 분산이란?**

캐시 용량을 늘리기 위해 여러 대의 **DB 서버로 확장시키려면 국소성(locality)을 고려해서 분산**시켜야 한다.

![Image](https://github.com/user-attachments/assets/9d773311-c3fc-4cc9-8ade-2e14a3e1c893)

> 데이터를 그대로 복제하는 것이 아니라, 
**데이터에 대한 액세스 패턴을 고려해서 분산**시키는 것을 국소성을 고려한 분산이라고 한다.
>
- 위 그림처럼 엑세스 패턴에 따라 분산한 경우,
  전체 엑세스를 고려했을 때보다 서버마다 캐싱되는 메모리를 절약할 수 있다.

## **파티셔닝**

국소성을 고려한 분산을 실현하기 위해서는 파티셔닝이라는 방법을 자주 사용한다.

### **파티셔닝이란?**

> 한 대 였던 DB 서버를 여러 대의 서버로 분할하는 방법을 말한다

### 분할 방법

**테이블 단위 분할**

![Image](https://github.com/user-attachments/assets/70d346c5-8800-479b-a5fa-a79c5964df5f)

테이블 단위로 분할하는 방법

예를 들어 entry, bookmark, tag, keyword 테이블이 존재할 때,

- entry, bookmark
- tag, keyword

로 분할해서 각기 다른 서버로 관리하도록 하는 분할 방법이다.

테이블 단위로 분할했으면 entry, bookmark 테이블로의 요청은 어떤 서버로
tag나 keyword로의 요청은 다른 서버로 보내 처리될 수 있도록 애플리케이션을 변경할 필요가 있다.

**테이블 데이터 분할**

![Image](https://github.com/user-attachments/assets/cfbad63b-8336-4918-91c4-58492fcac945)

특정 테이블 하나를 여러 개의 작은 테이블로 분할하는 방법이다.

예를 들어서 ID의 첫 문자로 파티셔닝을 하는 경우

- a~c인 경우는 서버 1
- d~f인 데이터는 서버 2

이런 식으로 분할하는 방법이다.

- ❗ **테이블 데이터 분할**의 문제점
    - 분할의 입도를 크거나 작게 조절할 때 데이터를 한 번 병합해야 한다는 번거로움이 있다.
    - 이 점을 제외하면
        - 애플리케이션에서 할 일은 ID의 첫 문자를 보고 액세스 할 DB서버를 분배하는 처리를 살짝 넣기만 하면 돼서 구현상으로는 간단하다.

## 요청 패턴을 '섬'으로 분할
용도별로 시스템을 섬으로 나누는 방식이다.
![Image](https://github.com/user-attachments/assets/033a0fca-4efa-409c-82cf-49146d477d13)

### **예를 들어**

HTTP 요청의 User-Agent나 URL 등을 보고 통상의 사용자면 섬 1, 일부 API 요청이면 섬 2, 검색 봇이면 섬 3과 같은 식으로 나누는 방법

> 검색 봇은 사람의 경우라면 좀처럼 액세스 하지 않을 아주 오래된 웹 페이지에도 액세스 하고, 광범위하게 액세스 한다. 이러한 경우 캐시가 작용하기 어렵다.
(동일한 페이지에 잇따라 방문하는 경우에는 캐시로 성능을 끌어올리기 쉽지만, 이처럼 광범위한 액세스에는 그럴 수가 없다)
>
- 검색 봇에 대한 엑세스 :  그렇게 빨리 응답할 필요는 없기 때문에 따로 섬으로 나눠 놓는다.
- 봇 이외에 액세스 (사용자로부터의 액세스):  특정 페이지들에 액세스가 집중되므로 빈번하게 참조되는 부분은 캐싱하기 쉽다.

이렇게 “캐싱하기 쉬운 요청/어려운 요청”을 처리하는 섬으로 나누게 되면,
전자는 국소성을 인해 안정되고 높은 캐시 적중률을 낼 수 있게 된다.

## **페이지 캐시를 고려한 운용의 기본 규칙**

1. OS 기동 직후에 서버를 투입하지 않는다.
    - 갑자기 배치하면 캐시가 없으므로 오직 디스크 액세스만 발생하게 된다.
    - 해결 방법으로는 OS를 시작해서 기동하면 자주 사용하는 DB의 파일을 한 번 cat 해준다.
      그렇게 하면 전부 메모리에 올라가므로 이후에 로드밸런서에 편입시킨다.
2. 성능평가나 부하시험 때는 캐시가 최적화되었을 때 한다.