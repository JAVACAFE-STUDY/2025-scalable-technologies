# 3장 - OS 캐시와 분산

> 대규모 데이터를 효율적으로 처리하는 원리
> 

# 8강 - OS의 캐시 구조

## OS의 캐시 구조를 알고 애플리케이션 작성하기 - 페이지 캐시

OS는 메모리를 이용해서 디스크 액세스를 줄인다. 원리를 알고 이를 전제로 애플리케이션을 작성하면 OS에 상당부분을 맡길 수 있다. 

그 원리가 바로 **OS 캐시**. Linux의 경우 페이지 캐시나 파일 캐시, 버퍼 캐시라고 하는 캐시 구조를 갖추고 있다. 여기서는 ‘**페이지 캐시**’라고 하겠다. Linux의 페이지 캐시의 특성을 확실히 알아두어야 한다.

### **Linux(x86)의 페이징 구조를 예로**

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image.png)

OS는 ‘**가상 메모리 구조**’를 갖추고 있다. 가상 메모리 구조는 **논리적인 선형 주소를 물리적인 물리 주소로 변환**하는 것이다. 

## 가상 메모리 구조

가상 메모리는 물리적인 하드웨어를 OS에서 추상화하기 위해서 사용된다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%201.png)

1. 메모리
2. OS
3. 애플리케이션 프로세스

메모리에는 `0x00002133`같은 32비트 **주소**가 붙어있다. 그러나 이 주소를 프로그램에서 직접 사용하면 안된다. 따라서 프로세스에서 메모리가 필요하면 직접 메모리의 주소를 가져오는 것이 아니라 OS에게 요청한다. OS는 메모리에서 비어있는 부분을 찾아서 실제(물리)와는 다른 가상의 주소를 반환한다. 

그 이유는 개별 프로세스에서는 메모리의 어느 부분을 사용하는지 관여하지 않고 ‘**반드시 특정 번지부터 시작**’ 또는 ‘**0x000부터 시작**’하면 메모리를 다루기 쉽기 때문이다. 주소를 매핑하는 것 외에도 가상 메모리에는 다양한 이점이 있지만 생략한다.

OS는 디스크를 읽을 때에 한 번에 4KB 정도 씩 읽어낸다고 했는데 메모리를 확보할 때도 마찬가지로 4KB정도를 블록으로 확보해서 프로세스에 넘긴다. 여기서 1개의 블록을 ‘**페이지**’라고 한다. OS는 프로세스에서 메모리를 요청받으면 페이지를 1개 이상, 필요한 만큼 확보해서 프로세스에 넘긴다.

## Linux의 페이지 캐시 원리

OS는 확보한 페이지를 메모리상에 계속 확보해둔다.

- **프로세스가 디스크에서 데이터를 읽는 과정**

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%202.png)

OS는 디스크에서 4KB 크기의 블록을 읽어낸다. 읽어낸 블록은 한 번은 메모리 상에 위치시킨다. 프로세스는 디스크에 직접 엑세스할 수 없기 때문이다. 프로세스가 액세스할 수 있는 것은 (가상) 메모리다. 그리고 OS는 그 메모리 주소를 프로세스에 **가상 주소**로 알려준다. 그러면 프로세스는 해당 메모리에 액세스하게 된다. 

데이터 읽기를 마친 프로세스가 더 이상 해당 데이터가 필요가 없어져도 메모리의 데이터를 해제하지 않고 남겨둔다. 그렇게 하면 다음에 다른 프로세스가 디스크의 같은 부분이 필요하면 디스크를 액세스할 필요없이 메모리에 남겨둔 페이지를 사용할 수 있다. 이것이 **페이지 캐시**다. 즉, 커널이 한 번 할당한 메모리를 해제하지 않고 계속 남겨두는 것이 페이지 캐시의 기본이다. 

### 페이지 캐시의 친숙한 효과

예외적인 상황을 제외하고 모든 I/O 상황에 적용된다. 즉 Linux에서는 디스크에 데이터를 읽으러 가면 꼭 한 번은 메모리로 가서 데이터가 반드시 캐싱된다. 따라서 2번째 이후의 액세스가 빨라진다. 현대의 OS는 대체로 페이지 캐시와 비슷한 구조를 갖추고 있다. OS를 계속 가동시켜 두면 메모리가 허락하는 한 디스크상의 데이터를 계속 캐싱하게 된다. 

Windows 머신의 경우 선뜻 재부팅하곤 하는데 사실 재부팅하지 않는 편이 디스크를 읽어낼 때 캐시가 작용하기 쉬우므로 속도는 빨라진다. 

## VFS - 가상 파일시스템

디스크의 캐시는 페이지 캐시에 의해 제공되지만, 실제 이 디스크를 조작하는 디바이스 드라이버와 OS 사이에는 **파일시스템**이 끼어 있다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%203.png)

리눅스에는 ext3, ext2, ext4, xfs 등 몇몇 파일시스템이 있고 그 하위에 디바이스 드라이버가 있으며 이 디바이스 드라이버가 실제로 하드디스크를 조작한다.

파일시스템 위에는 VFS(가상 파일시스템)이라는 추상화 레이어가 있다. 파일시스템은 다양한 함수를 갖추고 있는데, 그 인터페이스를 통일하는 것이 VFS의 역할이다. 또한 VFS가 페이지 캐시의 구조를 지니고 있다. 어떤 파일시스템을 이용하더라도, 어떤 디스크를 읽어내더라도 반드시 동일한 구조로 캐싱된다. 

## Linux는 페이지 단위로 디스크를 캐싱한다

‘파일 캐시’라는 이름은 적절하지 않는데, 그 이유를 설명한다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%204.png)

디스크에는 4GB의 파일이 있고, 메모리는 2GB다. 메모리의 500MB는 이미 사용되고 있기 때문에 여유 공간은 1.5GB다. 4GB 파일을 캐싱할 수 있을까?

‘파일 캐시’라고 생각하면 파일 1개 단위로 캐싱하고 있다는 이미지를 주므로 4GB는 캐싱할 수 없다고 생각할 수 있지만, 실제로는 그렇지 않다.

OS는 블록 단위만으로 캐싱할 수 있는 범위가 정해진다. 여기서는 디스크상에 배치되어 있는 4KB 블록만을 캐싱하므로 파일의 일부분만, 읽어낸 부분만을 캐싱할 수 있다. 이렇게 디스크를 캐싱하는 단위가 페이지다. 

### LRU

메모리 여유분이 1.5GB 있고 파일 4GB를 전부 읽게 되면 어떻게 될까? 구조상으로 LRU(Least Recently Used), 가장 오래된 것을 파기하고 가장 새로운 것을 남겨놓는 형태로 되어 있으므로 최근에 읽은 부분이 캐시에 남고 과거에 읽은 부분이 파기되어 간다. 따라서 DB도 계속 구동시키면 캐시가 점점 최적화되어 가므로 기동시킨 직후보다 점점 뒤로 갈수록 부하, I/O가 내려가는 특성을 보인다.

### (보충) 어떻게 캐싱될까? - i노드와 오프셋

Linux는 파일을 i노드 번호라고 하는 번호로 식별하며, 해당 파일의 **i노드 번호**와 해당 파일의 어느 위치부터 시작할지를 나타내는 **오프셋**, 이 두 가지 값을 키로 캐싱한다. 이 두 가지를 키로 하면 ‘어떤 파일을 어느 위치를’ 이라는 쌍으로 키를 관리할 수 있으므로 결과적으로 파일 전체가 아닌 파일의 일부를 캐싱할 수 있다. 

그리고 파일이 아무리 크더라도 이 키로부터 해당 페이지를 찾을 때의 데이터 구조는 최적화되어 있다. OS 내부에서 사용되고 있는 데이터 구조는 Radix Tree라고 하며, 파일이 아무리 커지더라도 캐시 탐색속도는 떨어지지 않도록 개발된 데이터 구조다. 따라서 커다란 파일의 일부분을 캐싱하거나 작은 파일의 일부분을 캐싱하더라도 동일한 속도로 캐시를 찾을 수 있도록 되어 있다.

## 메모리가 비어 있으면 캐싱 - sar로 확인해보기

실례를 보면서 페이지 캐시의 특성을 살펴보자. 우선 리눅스는 메모리가 비어 있으면 전부 캐싱한다. 프로세스가 메모리를 요청하면 캐시로 인해 더 이상 메모리가 남아있지 않다면 오래된 캐시를 버리고 프로세스에 메모리를 확보해준다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%205.png)

sar -r을 실행하면 1초에 한 번 현재 메모리의 상태를 출력한다. 

‘kbcached’는 kilo byte cached의 약자로, 캐시되어 있는 용량이다. 대략 1GB 중 700MB 가까이 캐시에 사용되고 있다. %memused(메모리 사용량)은 99%다. 메모리가 비어있는 곳에 OS가 조금씩 디스크를 캐싱하고 있는 것이다. 

## 메모리를 늘려서 I/O 부하 줄이기

메모리를 늘리면 더 많이 캐시할 수 있으므로 I/O 부하를 줄일 수 있다. 

- **하테나 북마크의 데이터**

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%206.png)

%iowait이 대략 20%다. 이는 프로세스가 작업을 수행할 때 항상 I/O에서 대기한다는 신호다. 

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%207.png)

메모리를 4GB에서 8GB로 늘리면 대기가 거의 없어졌다.

## 페이지 캐시는 투과적으로 작용한다

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%208.png)

부팅 직후 캐시로 데이터가 거의 유입되지 않았지만, 특정 파일을 read하면 이를 쭉 캐싱한다. 

- **sar 명령으로 OS가 보고하는 각종 지표 참조하기 - 서버/인프라를 지탱하는 기술 요약**
    
    sar 명령어 사용법
    

# 9강 - I/O 부하를 줄이는 방법

## 캐시를 전제로 한 I/O 줄이는 방법

캐시에 의한 I/O 경감효과는 매우 크다. I/O 대책의 기본이다. 이 기본으로부터 도출할 수 있는 포인트를 2가지 소개한다. 

첫 번째는 **데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있**으므로 이 점을 생각할 것. 다루고자 하는 데이터의 크기에 주목하자. 

또한 대규모 데이터 처리에는 데이터 압축이 중요하다고 했는데, 압축해서 저장해두면 디스크 내용을 저장해두면 디스크 내용을 전부 그대로 캐싱해둘 수 있는 경우가 많다. 예를 들어 LZ법 등 일반적인 압축 알고리즘의 경우, 압축률은 보통이더라도 텍스트 파일을 대략 절반 정도로 압축할 수 있다. 4GB의 텍스트 파일이라면 메모리 2GB인 머신으로 뒷부분 절반 정도는 거의 캐싱할 수 없었던 것이, 압축해서 저장해두면 2GB로 캐싱할 수 있는 비율이 상당히 늘어난다. 

두 번째는 **경제적인 비용과의 밸런스**를 고려하고자 한다는 점이다. 메모리의 가격이 낮아졌기 때문에 메모리를 늘려서 I/O부하를 해결하는 방법과 압축알고리즘을 개발하는 등 비용을 비교해서 더 경제적인 방식을 선택하자.

## 복수 서버로 확장시키기 - 캐시로 해결될 수 없는 규모일 경우

메모리를 늘려서 전부 캐싱할 수 있다면 좋겠지만, 당연히 데이터를 전부 캐싱할 수 없는 규모가 될 수 있다.   **복수 서버로 확장**시키는 방안을 생각해볼 필요가 있다. 

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%209.png)

프록시, AP 서버, DB 서버 3단 구조에 대한 복습이다. AP 서버를 늘려야 하는 이유는 기본적으로 CPU 부하를 낮추고 분산시키기 위해서다. 한 편 DB 서버를 늘려야 할 때는 반드시 부하 때문만은 아니고 오히려 **캐시 용량을 늘리고자 할 때 혹은 효율을 높이고자 할 때**인 경우가 많다.

AP 서버를 늘리는 것과 DB 서버를 늘리는 것은 둘 다 서버를 늘리는 것이지만 필요한 리소스, 요구되는 리소스가 전혀 다르다. DB 서버는 늘리면 좋다라는 논리가 들어맞지 않는다. 

## 단순히 대수만 늘려서는 확장성을 확보할 수 없다

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2010.png)

캐시 용량을 늘려야 한다고 했는데, 사실 단순히 대수를 늘리는 것만으로는 안 된다. 그림처럼 단순히 데이터를 복사해서 대수를 늘리게 되면 애초에 캐시 용량이 부족해서 그 부족한 부분도 그대로 동일하게 늘려가게 된다. 검은 부분이 변함없이 캐싱되지 않는 상황이 된다.

## 컬럼 : I/O 부하 줄이기와 페이지 캐시

리눅스는 메모리 영역을 4KB 블록단위로 관리한다. 이 4KB 블록은 ‘페이지’라고 한다. 페이지 캐시는 이름 그대로 페이지의 캐시다. 즉 디스크에서 데이터를 읽는 것은 페이지 캐시를 구축하는 것이다. 읽어낸 데이터는 페이지 캐시에서 사용자 공간으로 전송된다.

리눅스의 페이지 캐시 동작원리에서 기억해야 할 것은 ‘리눅스는 가능한 한 남아있는 메모리를 페이지 캐시로 활용하려고 한다’는 원칙이다. 디스크에서 읽은 데이터는 캐시로 메모리에 남겨둔다. 

### 페이지 캐시에 의한 I/O 부하의 경감 효과

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2011.png)

MySQL 데이터의 용량은 20GB로 메모리가 16GB면 유효한 데이터는 거의 캐시에 올릴 수 있다. 따라서 iowait은 거의 없다시피 사라졌다. 

sar -r을 보면 커널이 캐시를 어느 정도 확보하고 있는지 판단할 수 있다. 캐시 용량과 실제로 애플리케이션이 다루는 유효한 데이터량을 비교해서 데이터량이 많을 경우 메모리 증설이 검토한다. 

메모리를 증설할 수 없을 경우에는(분산이 더 경제적인 경우?) 데이터를 분할해서 각각의 서버에 위치시키는 것을 검토한다. **데이터를 적절하게 분할하면 단순히 서버 대수를 늘린 만큼 디스크 I/O 횟수만 줄어드는 것이 아니라, 캐시에 올릴 데이터의 비율도 늘어나므로 상당한 전송량 향상을 기대할 수 있다**.

### 페이지 캐시는 한 번의 read에서 시작된다

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2012.png)

MySQL 서버를 기동한 직후 20분 정도 지난 시점. 기동 후 데이터 파일 전체를 읽어들이는 프로그램을 실행했더니 메모리 사용률이 97%까지 증가했다. 페이지 캐시로 사용되고 있는 것을 알 수 있다. 

# 10강 - 국소성을 살리는 분산

## 국소성을 고려한 분산이란?

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2013.png)

DB서버에 액세스 패턴 A일 때는 1로 액세스가 많이 오고 패턴 B일 때는 2로 오는 것처럼, 데이터로 액세스하는 경향에 대한 처리방식에 따라 특정한 방향으로 치우치는 경우가 자주 있다.

데이터에 대한 액세스 패턴을 고려해서 분산시키는 것을 국소성을 고려한 분산이라고 한다.

예를 들어 하테나 북마크를 보면 ‘인기 엔트리’ 페이지를 표시하는 경우에는 인기 엔트리용 DB의 캐시 테이블을 많이 액세스한다. 자신의 북마크 테이블에 액세스하는 경우 다른 테이블을 액세스하기 때문에 두 액세스 패턴은 전혀 다르다. 인기 엔트리로 액세스할 때는 서버 1로, 그렇지 않을 때는 서버 2로 요청을 분배한다. 그러면 더 이상 서버 1에 있는 액세스 패턴 B을 위한 캐시를 유지할 필요가 없기 때문에 온전히 액세스 패턴 A를 위해서 캐시를 사용할 수 있다. 결국 시스템 전체로서는 메모리에 올라간 데이터량이 늘어나게 된다. 

## 파티셔닝 - 국소성을 고려한 분산 1

국소성을 고려한 분산을 실현하기 위해서는 파티셔닝이라는 방법을 자주 사용한다. 파티셔닝은 한 대였던 DB서버를 여러 대의 서버로 분할하는 방법을 말한다. 분할 방법은 여러 가지 있겠지만, 간단한 것은 ‘**테이블 단위 분할**’이다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2014.png)

entry, bookark와 tag, keyword를 분할해서 각기 다른 서버로 관리한다. 

테이블 단위로 분할했으면 entry나 bookmark 테이블로의 요청은 첫번째 서버로, tag, keyword 테이블로의 요청은 두 번째 서버로 보내 처리될 수 있도록 애플리케이션을 변경해야 한다.

다른 분할 방법으로는 ‘**테이블 데이터 분할**’이 있다.

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2015.png)

특정 테이블 하나를 여러 개의 작은 테이블로 분할한다. 하테나 다이어리에서는 실제로 이 분할 방법을 사용하고 있으며, 구체적으로는 ID(id:~)의 첫 문자로 파티셔닝하고 있다. 

이 분할의 문제점은 분할하는 서버의 수를 조정하기 위해서는 데이터를 한 번 병합해야 하기 때문에 번거롭다. 이 점을 제외하면, 애플리케이션에서 할 일은 ID의 첫 문자를 보고 액세스할 DB 서버를 분배하는 처리를 넣기만 하면 된다.

## 요청 패턴을 ‘섬’으로 분할 - 국소성을 고려한 분산 2

‘용도별로 시스템을 섬으로 나누는 방법’도 있다. 이는 하테나 북마크에서 사용하고 있는 방법이다. 

![image.png](3%E1%84%8C%E1%85%A1%E1%86%BC%20-%20OS%20%E1%84%8F%E1%85%A2%E1%84%89%E1%85%B5%E1%84%8B%E1%85%AA%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%201bf431715117805e932fdf44e57eaa36/image%2016.png)

하테나 북마크에서는 HTTP 요청의 User-Agent나 URL 등을 보고, 예를 들어 통상의 사용자면 섬1, 일부 API 요청이면 섬3, bot이면 섬2와 같은 식으로 요청을 분배한다.

검색 봇은 특성상 아주 오래된 웹 페이지에도 액세스하러 온다. 사람의 경우라면 좀처럼 액세스하지 않을 페이지에도 액세스하고, 광범위하게 액세스한다. 그렇게 되면 캐시가 작용하기 어렵다. 봇의 요청을 빠르게 응답할 필요가 없기 때문에 섬으로 나눠놓는다. (봇의 요청이 그렇게 많나? 봇 때문에 캐시미스가 많이 나서 효율이 떨어지나?) (주석에서 구글에서 봇 응답 속도도 검색 랭킹 평가에 반영한다고 하네.)

한편 사용자로부터의 액세스는 최상위 페이지나 인기 엔트리 페이지 등 최신, 인기 페이지에 거의 액세스가 집중되므로 빈번하게 참조되는 부분은 캐싱하기 쉽다.

이렇게 해서 캐싱하기 쉬운 요청, 캐싱하기 어려운 요청을 처리하는 섬을 나누게 되면, 전자는 국소성으로 인해 안정되고 높은 캐시 적중률을 낼 수 있다. 후자의 요청이 전자의 캐시를 어지럽히므로 섬으로 나누는 경우에 비해 전체적으로는 캐시 효율이 떨어진다. (봇이 캐시 효율을 떨어트릴 정도로 요청이 많이 들어오려나?)

하테나 북마크는 사람에 의한 요청보다 봇에 의한 액세스가 더 많다. 하테나 북마크는 구조상 내부 링크가 매우 많으므로 링크를 따라가는 봇은 좀처럼 순회를 멈추지 못하는 문제가 있어서 섬으로 나눠놓아야 한다.

하테나 북마크의 웹 API에는, 예를 들어 ‘북마크 수가 몇 건인지 반환’하는 API가 있는데, 이는 정해진 테이블만 액세스한다. 그 부분만 캐싱이 잘 적용되도록 별도의 섬으로 나누는 것도 유효하다. 하테나 북마크 건수를 조회하는 API도 제공하는 데 이 API에는 상당한 요청이 있다. (모니터를 잘해서 어떻게 하면 효율적으로 분산할 수 있을지 고민을 많이 해야할듯..)

## 페이지 캐시를 고려한 운용의 기본 규칙

페이지 캐시와 관련해서 운용면에서도 생각해야 할 부분이 있으므로 파악해두자. 

1. OS 기동 직후에 서버에 투입하지 않는 것. 기동 직후에는 캐시가 없기 때문에 많은 요청을 받으면 서버가 죽는다. 기동 직후에 자주 사용하는 DB의 파일을 한 번 cat해서 메모리에 데이터를 올리고(웜업) 로드밸런서에 편입한다. 
2. 성능평가나 부하시험을 할 때는 캐시가 최적화된 후에 실시한다.

## 컬럼 : 부하분산과 OS의 동작원리

부하분산에서 가장 도움이 되는 지식은 OS 동작원리다. OS의 동작원리를 배워보면, 

- OS 캐시
- 멀티스레드나 멀티프로세스
- 가상 메모리 구조
- 파일시스템

등 다양한 장치가 하드웨어를 효율적으로 사용하기 위해 어떤 원리를 갖추고 있는지를 비롯해 장점과 단점을 확실히 알 수 있다. 이런 점들을 알게 되면 OS의 장단점과 함께 시스템 전체를 최적화할 수 있다.