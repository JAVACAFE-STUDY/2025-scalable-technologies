# 7장 - 알고리즘 실용화

가까운 예로 보는 이론, 연구의 실전 투입

# 19강 알고리즘과 평가

## 데이터 규모와 계산량 차이

데이터가 클수록 알고리즘이나 데이터 구조가 속도에 미치는 영향이 크다. 

- n = 1억
    - O(n): 1억
    - O(log n): 30

### 제 7장, 두 가지 목적

1. 대규모 데이터 처리에서 알고리즘 선택의 중요성 이해. 단순히 알고리즘 구현을 바꾸는 것만으로는 해결되지 않는다.
2. 이론적 알고리즘을 실제 제품에 적용하는 과정 탐구. 교과서나 논문에서 다루지 않는 실제 시스템 구현과 운용 방법 설명

이론적 알고리즘을 실제 제품에 성공적으로 적용하기 위해서는 구현 이상의 다양한 고려사항이 필요하다.

## 알고리즘이란?

<aside>
💡

적당한 값을 입력하면 명확하게 정의된 계산절차에 따라 값이 출력으로 반환되는 것

</aside>

탐색하고자 하는 값과 대상 데이터를 입력하면 탐색이 이뤄져서 목적 데이터의 위치가 반환된다. 이것은 ‘탐색 알고리즘'이다.

### 좁은 의미의 알고리즘, 넓은 의미의 알고리즘

1. 넓은 의미의 알고리즘:
    - 일반적인 프로그램의 처리 흐름을 지칭
    - 데이터베이스 처리, 도메인 로직 등 일상적인 프로그래밍 과정
    - 비즈니스 로직이나 처리 흐름을 설명할 때 사용
2. 좁은 의미의 알고리즘:
    - 명확하게 정의된 계산 문제에 대한 구체적인 계산 절차
    - 정렬, 탐색, 해시법 등 특정 계산 문제의 해법
    - 알고리즘 교재에서 주로 다루는 내용

7장에서는 좁은 의미의 알고리즘에 중점을 둔다.

## 알고리즘을 배우는 의의

- 컴퓨터의 자원은 유한, 엔지니어의 공통언어

**알고리즘 학습의 기본적 필요성:**

컴퓨터의 자원(CPU, 메모리)은 유한하므로 효율적인 사용이 중요하다. 한정된 자원으로 문제를 해결하는 방법을 익히는 것이 필요하다.

**커뮤니케이션 도구로서의 가치:**

디자인패턴과 같이 엔지니어들 간의 공통 언어 역할이다. 효율적인 의사소통을 위해 알고리즘에 대한 공통된 이해 필요하다.

- 새로운 문제 해결에 적용 가능 (예: 베이지안 필터를 활용한 스팸 필터 개발)
- 효율적인 데이터 구조 활용 (예: Google 일본어 입력기의 LOUDS 구조 활용) 50MB로 압축.
- 대규모 데이터 처리 시 성능 최적화에 크게 기여

## 알고리즘의 평가

- Order 표기

- O(1): n의 크기에 관계없이 일정한 시간에 처리가 끝남. (예: 해시함수)
- O(n): 계산량과 n의 크기가 같은 경우. (예: 선형탐색)
- O(log n): 계산을 할 때마다 결과가 반 씩 줄어드는 경우. (예: 이진탐색)

### 각종 알고리즘의 Order 표기

<aside>
💡

O(1) < O(log n) < O(n) < O(n log n) < O(n^2) < O(n^3) ··· O(n^k) < O(2^n)

</aside>

상당히 복잡한 알고리즘을 O(n^2)으로 계산할 수 있을 경우 '이건 빠르다' 라고 주장할 수도 있으므로 어디까지나 대상이 되는 계산에 따라 다를 수 있다. 예를 들면 **일반적인 정렬 알고리즘은 아무리 잘해도 O(n log n)보다 빠를 수는 없다는 것이 이론적으로 증명되어 있다**. 따라서 정렬 알고리즘에서는 O(n log n)이면 빠르다고 할 수 있을 것이다.

(생략)

## 알고리즘과 데이터 구조

 - 뗄래야 뗄 수 없는 관계?!

데이터 구조와 알고리즘이 세트로 논의되는 것은 알고리즘에서 자주 사용하는 조작에 맞춰 데이터 구조를 선택할 필요가 있기 때문이다. 예를 들면 사전에 적절한 트리구조로 데이터를 저장해두면, 대개의 경우는 탐색처리를 단순화할 수 있어 계산량을 줄일 수 있다. (예: B+트리)

## 계산량과 상수항

 - 역시 측정이 중요

상수항은 입력 크기와 무관한 필수 처리 과정들이다. 함수 호출, 반환, 변수 할당, if문 분기 등이 해당한다. Order 표기(빅오 표기법)에서는 일반적으로 무시된다.

상수항의 실제적 영향은 단순한 구현에서는 영향이 미미하지만 복잡한 구현에서는 무시할 수 없는 영향을 미친다. CPU 캐시 활용도, 분기 예측 등 하드웨어 특성에 따라 차이 발생한다.

여러 O(n log n) 정렬 알고리즘 중, 퀵정렬이 일반적으로 가장 빠른 이유는 CPU 캐시 활용이 효율적이기 때문이다. 같은 계산량 Order라도 실제 성능은 다를 수 있다. 상수항이 다르기 때문이다.

## 알고리즘의 실제 활용

 - 단순한 게 더 낫기도?

잘 알려지거나 고도의 알고리즘이 항상 최선은 아니다. 고전적이거나 단순한 알고리즘이 더 나은 경우도 많다.

### 하테나 북마크 Firefox 확장기능인 검색기능에서의 시행착오

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image.png)

브라우저에 하테나 북마크를 통합하는 Firefox 확장기능에는 사용자가 북마크한 데이터를 증분검색할 수 있는 기능이 있다. 증분검색은 검색 빈도도 높고 클라이언트에서 계산되기 때문에 계산량을 줄이기 위해서 Suffix Array를 사용하기로 했다. Suffix Array는 탐색은 빠르지만 미리 전처리 된 데이터 구조를 만들어 둬야한다. 전처리할 수 있는 최신 알고리즘도 사용해서 기능을 구현해보니 결과가 만족스럽지 못해서 확장기능이 내부적의 SQLite에 like 조건으로 구현했다. 이렇게 단순하게 구현해도 문제가 없었다.

## 써드파티 소스를 잘 활용하자

 - CPAN 등

오픈소스를 잘 사용하자. 

- **컬럼: 데이터 압축과 속도**
    
    데이터를 압축하면 처리량이 향상되는 경우가 많다. 파일을 압축해두면 CPU에 조금 부담이 되지만 I/O대기를 줄일 수 있다. CPU가 여유 있고 I/O가 바쁜 경우가 많으므로 압축으로 인해 I/O 부하를 줄이고 CPU에 사용률을 올려서 전체적인 처리량을 높일 수 있다.
    

# 20강 하테나 다이어리 키워드 링크

## 키워드 링크란?

블로그 서비스인 하테나 다이어리에는 **키워드 링크**라는 약간 특이한 기능이 있다. 

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%201.png)

블로그에 글을 작성하면 일부 키워드에 링크가 자동으로 걸린다. 링크는 키워드를 설명하는 페이지. wiki의 기능과 비슷.

링크 대상 키워드는 하테나 키워드에 사용자가 등록한 키워드다. 

입력된 전문에 대해 27만 단어를 포함하는 키워드 사전과 매칭해서 필요한 부분을 링크로 치환하는 것이 키워드 링크의 기능이다.

## 최초 구현방법

처음에는 사전 내에 포함된 모든 단어를 OR 조건으로 잇는 정규표현으로 구현했다. (예: `foo|bar|baz`)

![조건에 맞는 키워드를 추출해서 해당 키워드에 링크를 거는 코드](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%202.png)

조건에 맞는 키워드를 추출해서 해당 키워드에 링크를 거는 코드

## 문제발생! - 키워드 사전의 대규모화

키워드 수가 많아지면서 정규식을 **컴파일**하고 **패턴을 매칭**하는 시간이 길어지는 문제가 생겼다. 컴파일은 미리 정규식을 만들어서 캐싱을 해두면 되었지만, 패턴매칭은 문제는 그렇지 않았다.

## 패턴매칭에 의한 키워드 링크의 문제점

키워드 수가 늘고 하테나 다이어리의 액세스수도 늘어가면서 시스템의 문제가 커지고 있었다. 패턴매칭 시 시간이 걸리는 이유는 정규식의 알고리즘 때문이었다. 정규식은 패턴매칭 구현에 **오토마톤**을 사용한다. 그리고 Perl의 정규식은 **NFA**를 사용한다. NFA는 앞서 본 (foo|bar|baz)와 같은 패턴을 하나 씩 순회하면서 처리하기 때문에 키워드 개수에 비례하는 계산량이 소요된다.

## 정규표현 → Trie - 매칭 구현 변경

패턴 매칭에 수반되는 계산량 문제를 해결하기 위해 정규표현을 기반으로 한 방법에서 Trie를 사용한 매칭 구현으로 변경했다.

### Trie 입문

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%203.png)

Trie는 트리의 일종인 데이터 구조다. “abcde”를 트라이로 나타내면 다음과 같다. 각 문자를 노드로 만든다. [a, b, c, d, e] a부터 e까지 노드를 순서대로 연결한다. 그리고 e노드(단어 마지막)에 해당 노드가 마지막 노드라는 표시를 해둔다. 그리고 “ab”를 추가하는 경우 이미 a → b라는 노드가 있기 때문에 새로운 노드를 추가하지 않고 b노드에 마지막 노드라는 표시를 추가만하면 “ab”가 추가된다.

### Trie 구조와 패턴매칭

Trie를 사용하면 정규식으로 단어마다 순회하며 검색하지 않고 접두사가 같은 단어들은 크게 계산량이 줄어든다.

## AC법 - Trie에 의한 패칭을 더욱 빠르게

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%204.png)

하테나 다이어리를 개선했을 때 Trie 구조에 의한 패턴매칭을 더욱 빠르게 한 Aho-Corasick(AC법)을 이용했다. 계산량이 사전 크기에 의존하지 않는 빠른 방법이다.

AC법은 Trie에서 패턴매칭으로 매칭을 하다가 도중에 실패하는 경우, 되돌아오는 길의 엣지를 다시 Trie에 추가한 데이터 구조를 사용하는 방법이다.

예를 들어 Trie에 “babcdes”를 입력한 경우 bab가 발견된 다음은 ab가 발견될 거라는 걸 알고 있다. 그러므로 bab까지 탐색했다면 다시 노드 0으로 돌아가지 않고 바로 노드 2로 갈수 있다는 걸 알 수 있다면 ab를 바로 찾을 수 있다. 여기서 ‘6 다음은 바로 2’라고 길을 내는 전처리를 Trie에 대해 수행하는 것이 AC법의 핵심이다. Trie의 루트부터 너비 우선탐색으로 적당한 노드를 찾아감으로 구성할 수 있다.

## Regexp::List로의 치환

AC법을 채택함으로 키워드 링크의 계산량 문제는 해결했다. 추후 Regexp::List라는 CPAN 라이브러리를 사용했다. Regexp::List는 Trie에 의한 최적화된 정규표현으로 변환하는 게 이 라이브러리다. 

Regexp::List의 이점은 계산량이 줄어들 뿐 아니라 정규식을 사용할 수 있다는 점이다. 

## 키워드 링크 구현, 변이 및 고찰

키워드 링크의 구현은 거대한 정규표현 → AC법 → Regexp::List로 변화했다. 이 과정에서 알게된 점이 몇 가지 있다.

처음 기능 구현 시 간단하게 구현했던 것이 장점이기도 했다. 공수도 적고 유연했다. 데이터가 커짐에 따라 발생한 문제점에 대한 근본적인 문제점을 해결해야 했다. 알고리즘의 계산량이 문제였다.

# 21강 하테나 북마크의 기사 분류

## 기사 분류란?

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%205.png)

하테나 북마크에서는 새로 도착한 기사를 해당 기사의 내용을 기반으로 사용자에게 카테고리를 분류해서 3보여주는 기능을 제공한다. 

### 베이지안 필터에 의한 카테고리 판정

이 카테고리 판정에는 베이지안 필터를 사용하고 있다. 베이지안 필터는 텍스트 문서 등을 입력받아 나이브 베이즈라고 하는 알고리즘을 적용해서 확률적으로 해당 문서가 어느 카테고리에 속하는지 판정하는 프로그램이다. 특징은 미지의 문서의 카테고리 판정 시 과거에 분류가 끝난 데이터의 통계정보를 보고 판정한다. 

## 기계학습과 대규모 데이터

많은 기계학습 태스크에는 베이지안 필터와 같이 정해 데이터를 필요로 한다. 기계학습 태스크에 따라 데이터가 많을수록 정밀도가 향상되는 경우도 드물지 않다. 

### 하테나 북마크의 관련 엔트리

하테나 북마크에는 ‘관련 엔트리’라는 기능이 있다. 특정 기사와 매우 비슷한 정보를 사용자에게 제시하는 기능이다. 

![image.png](7%E1%84%8C%E1%85%A1%E1%86%BC%20-%20%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%80%E1%85%A9%E1%84%85%E1%85%B5%E1%84%8C%E1%85%B3%E1%86%B7%20%E1%84%89%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%AA%201cf43171511780049abad7f3d00ee33a/image%206.png)

이 기능은 하테나 북마크가 가지고 있는 4천만건 이상의 ‘태그’라는 분류용 텍스트를 입력으로 기사추천 알고리즘을 사용해서 구현한다. 대량 데이터로부터 의미있는 데이터를 추출해서 보여주는 것은 대규모 데이터를 갖는 웹 서비스에서만 할 수 있는 일일 것.

## 대규모 데이터와 웹 서비스 - The google Way of Science

대규모 데이터를 쏟아부으면 블랙박스에서 정답이 나온다는 사례를 들고 있다. (LLM까지 왔다.)

## 베이지안 필터의 원리

베이지안 필터의 핵심은 ‘**나이브 베이즈 알고리즘**’이다. 베이즈의 정리라는 공식을 기반으로 한다.

### 나이브 베이즈에 근거한 카테고리 추정

특정 문서 D가 카테고리 C에 속할 확률을 구하는 문제다. 

- `P(C|D)`

여러 카테고리 중 확률이 가장 높은 C가 최종적으로 선택된다. 조건부 확률을 직접 계산하는 것은 어렵지만 베이즈 정리에 따라 계산 가능한 식으로 변형할 수 있다. 베이즈의 정리 자체가 원래 그렇다라고 하기보다 잘 알려진 수리로 확률식을 변형할 수 있다는 점이 포인트다. 

- `P(CID) = P(DIC) P(C) / P(D)`

변형하는 위와 같이 되고, 우변의 각 확률`P(DIC), P(C), P(D)`을 구하는 문제가 된다. `P(D)`는 문서 D가 발생할 확률인데, 이는 모든 카테고리에 대해 동일한 값으로 결과를 비교할 경우에는 무시할 수 있다.

`P(DIC), P(C)` 두 가지만 알면 된다. `P(C)`는 특정 카테고리가 출현할 확률이므로 학습 데이터 중 여러 데이터가 어떤 카테고리로 분류되었는지 횟수를 저장해두면 나중에 계산할 수 있다.

`P(DIC)`는 문서 D라는 것은 임의의 단어 W가 연속해서 출현하는 것으로 간주하고 `P(DIC) → P(W1|C), P(W2|C), P(W3|C)…P(Wn|C)`와 같은 식으로 근사해 볼 수 있다. 문서 D를 단어로 분할해두고 그 단어마다 어느 카테고리로 분류됐는지 횟수를 보존해두면 `P(DIC)`의 근사값을 구할 수 있다.

### 손쉬운 카테고리 추정 실현

결국, 나이브 베이즈에서는 정해 데이터가 주어지면 해당 정해 데이터가 사용된 횟수나 단어 출현횟수와 같이 간단한 수치만 저장해두면 나중에 확률만 계산하면 카테고리를 추정할 수 있다. 

‘대규모 기사군의 내용을 보고 각각의 카테고리를 자동으로 판정하라’는 막막하겠지만 알고리즘의 원리까지 분해해보면 의외로 간단하게 구현할 수 있음을 알 수 있다.

## 알고리즘이 실용화되기까지 - 하테나 북마크의 실제 사례

베이지안 필터는 알고리즘 구현 자체는 간단한 편이다. 베이지안 필터로 만든 카테고리 분류 엔진을 프로덕션 환경에 올리기까지 어떤 작업들이 남았는지 살펴본다. 

- 분류 엔진은 C++ 로 개발했다. 이 엔진을 서버화한다.
- 이 서버와 통신해서 결과를 얻는 Perl 클라이언트를 작성하고 웹 애플리케이션에서 호출한다.
- 학습 데이터를 정기적으로 백업할 수 있도록 C++ 엔진에 데이터 덤프/로드 기능을 추가한다.
- 학습 데이터 1,000건을 수작업으로 준비한다. 이 부분은 사람이 애써야 하는 부분이다.
- 바람직한 정밀도가 나오는지를 추적하기 위한 통계 구조를 작성한다. 그래프화하면서 정밀도를 튜닝한다.
- 다중화를 고려해서 스탠바이 시스템을 구축한다. 자동 페일오버(fail-over)는 역시나 공수가 많이 소요되므로 백업에서 로드할 수 있는 정도로 타협한다.
- 웹 애플리케이션에 사용자 인터페이스를 마련한다.

## 수비 자세, 공격 자세 - 기사 분류 구현으로부터의 고찰

동일한 알고리즘이라도 대규모 데이터를 빠르게 정렬하거나 검색, 압축하는 일은 아무래도 발생하는 문제를 얼마나 잘 맞아들이는가라는 ‘수비’적인 자세해서 자주 사용하는 알고리즘. 반면, 기계학습이나 패턴인식 등은 적극적으로 대규모 데이터를 응용하고 그 결과에 따라 애플리케이션에 부가가치를 추가한다는 의미로 ‘공격’적인 자세로 사용하는 알고리즘.

### 기존 방법 익혀두기

대량의 데이터를 다루며 마주하는 문제를 해결하기 위해서는 알고리즘에 대한 지식이 필요하다. 

### 스펠링 오류 수정기능 만드는 법

1. 정해 데이터로는 27만 단어 정도의 하테나 키워드를 사전으로 사용한다.
2. 사용자가 입력한 검색쿼리와 사전 내의 어구 사이의 편집거리를 구해서 "오류 정도"를 정량화한다.
3. 일정한 오류 정도를 기준으로 사전 내에 있는 단어군을 정해 후보로 얻어낸다.
4. 3의 정해 후보를 하테나 북마크 기사에서의 단어 이용빈도를 기준으로 정해에 가까운 순으로 정렬한다.
5. 가장 이용빈도가 높은 단어를 정해로 간주하고 이용자에게 제시한다.