# 19. 알고리즘과 평가

## 데이터 규모와 계산량의 차이

대상이 되는 데이터가 크면 클수록 알고리즘이나 데이터 구조 선택이 속도에 큰 영향을 미친다.

### ❗ 예를들어

- 선형탐색
  - n건에 대해서 n번의 탐색이 필요
  - 1000건에 대해서 최대 1000번의 탐색이 필요
- 이분탐색
  - n건에 대해서 log n번의 탐색이 필요
  - 1000건에 대해서 최대 10번의 탐색이 필요


최대 탐색 횟수는 계산횟수의 기준이 되는 수로
`계산량` 이라고 한다.

n = 1000 일 때 계산량

- O(n) → 1000
- O(log n) → 10

n - 1000만 일 때 계산량

- O(n) → 1000만
- O(log n) → 24

대규모 데이터를 전제로 했을 때,
적절한 알고리즘의 선택으로 인한 차이가 점점 커지는 것을 볼 수 있다.

## 알고리즘이란?

- 좁은 의미의 알고리즘
  - 적당한 값을 입력하면 명확하게 정의된 계산절차에 따라 값이 출력으로 반환되는 것
- 넓은 의미의 알고리즘
  - 도메인 로직의 흐름

## 알고리즘을 배우는 의의

- 지금 해결해야하는 문제를 유한한 자원으로 어떻게 효율적으로 해결할 수 있을 지 알 수 있다.
- 배워두면 새로운 문제에도 대처 할 수 있다.
  - ex) 베이지안 필터를 구현하는 알고리즘을 알아두면
    데이터를 자동 분류하는 프로그램을 작성 할 수 있다.
- 데이터 규모가 커질 수록 적절한 알고리즘 사용으로 인한 성능향상을 기대해 볼 수 있다.

## 알고리즘의 평가

알고리즘의 계산량은 정량적으로 평가할 수 있다.
여기서 Order 표기법을 사용한다.
입력 크기가 n 일 때 대략적으로 이정도 계산량이 소요된다는 것을 표기하는 기법이다.

n의 크기에 관계 없이 일정= 시간에 처리가 끝나는 경우 `O(1)`이라고 쓴다.

- ex) 해시로 부터 데이터를 탐색할 경우,
  해시함수 계산은 n에 의존하지 않으므로 O(1)이다.

### 각종 알고리즘의 Order 표기

> O(1) < O(logn) < O(n) < O(nlogn) < O(n^2) < O (n^3) <...< O(n^k) < O(2^n)
>
- 오른쪽으로 갈수록 계산량이 많아진다.
- n이 클 경우 실질적으로 실용성을 띄는 것은 O(nlogn)부근이고,
  그 이상은 계산이 안끝나는 느낌을 주는 경우가 있다
- O(logn)은 O(n)에 비해 상당히 빠르고,
  O(n) 과 O(nlogn) 사이에는 그다지 차이가 없고 O(n),
  O(n^2)은 계산이 끝나고 끝나지 않는 정도의 격차가 있다
- 하지만 대상이 되는 계산에 따라 또 다르다
  - 정렬은 O(nlogn) 보다 빠를 수 없다는 것이 증명돼있으므로 O(nlogn)이면 빠른 것이다

### 지수적, 대수적 알고리즘 계산량에 대한 감각

- 계산량이 지수적으로 증가하는 알고리즘은 데이터량이 적어도 계산량이 매우 커져버린다.
  - O(n^2)
- 계산량이 대수적으로 증가하는 알고리즘은 데이터량이 커져도 적은 계산량으로 문제를 해결 할 수 있다.
  - O(log n)

## 알고리즘과 데이터 구조

알고리즘과 데이터 구조는 뗄래야 뗄 수 없는 관계이다.

- 왜냐하면, 알고리즘에서 자주 사용하는 조작에 맞춰 데이터 구조를 선택할 필요가 있기 때문이다.

❗ 예를 들어

RDBMS에서의 인덱스를 구현할 때 `B+트리` 라는 데이터 구조가 사용된다.
`B+트리`로 인덱스를 저장해두면,

- 탐색에 수반되는 단계 횟수도 조금 줄일 수 있고 디스크 IO 횟수도 최소화 할 수 있다

RDBMS에서는 `B+트리` 구조에 맞는 알고리즘으로 탐색, 삽입, 정렬 등을 수행한다.

## 계산량과 상수항

Order 표기는 상수항을 무시한다

- ex) 함수호출이나 함수로 부터 값을 반환하기 위한 처리

간단한 구현에서는 괜찮지만
하지만 복잡한 구현이 되면 상수항을 무시할 수 없게 된다.
구현이 복잡하지 않더라도 캐시에 올리기 쉬운지, 분기 예측이 발생하지 않는지 등 계산량의 구조적인 특성에 의존하는 형태로 상수항에서 차이가 많이 나는 경우도 많다

- ex) 정렬 알고리즘은 O(nlogn)이 하한으로, 계산량이 O(nlogn)인 알고리즘은 여러개 존재한다.
  - 하지만 같은 O(nlogn)이여도 일반적으로는 퀵 정렬이 가장 빠르다.
  - 왜냐하면 퀵정렬 특성상 CPU 캐시를 사용하기 쉽다는 장점이 있어서이다.
    → 이는 상수항이 빠르다는 것을 의미한다.

즉, Order 표기는 알고리즘을 비교할 때는 편리하지만 구현을 포함해서 생각할 때 그게 전부는 아니라는 것이다
그리고 상수항은 어떻게 구현하느냐에 의존하는 경우가 많아서 이를 줄이기 위해서는 구현에 노력을 기울여야 한다.

### 하지만!  ( 구현 시 유의했으면 하는 최적화 이야기 )

상수항을 줄이기 위해 처음부터 최적화를 수행하는 것은 잘못된 방침이다

- ex) O(n^2)을 노력해서 상수항을 줄이는 것보다
  O(nlogn)인 알고리즘으로 대체하는 것이 훨씬 이득이다.,

결국 측정이 중요하다는 것으로,
벤치마크를 하거나 프로파일링을 해서
지금 대상으로 하는 프로그램에 무엇이 문제인지를 정확히 아는 것이 중요하다.

- 알고리즘을 개선해야 할 지
- 상수항을 줄여서 개선해야 할 지
- 물리적 리소스를 확보하기 위해 하드웨어를 교환해 성능을 개선해야 할 지… 등등

## 알고리즘의 실제 활용 - 단순한게 더 낫기도?

고도의 알고리즘이 반드시 최고의 해법인 것은 아니고, 고전적 알고리즘이 좋을 수도 있다.

### 하테나 북마크 Firefox 확장 기능인 검색기능에서의 시행착오

하테나에서는 브라우저와 하테나 북마크를 통합해서 사용할 수 있는 firefox 확장프로그램을 제공했다.
이 확장 기능에는 과거에 사용자 본인이 북마크한 데이터를 증분 검색할 수 있는 기능이 있었다.

- 증분검색이라면 검색도 상당한 빈도로 발생하고
  클라이언트에서만 계산되니 계산량을 적게 가져가야 한다고 판단했음
- 데이터량이 사람에 따라서는 1만건 이상 됨

→  Suffix Array를 사용하겠다고 판단

- Suffix Array : 텍스트 데이터 등을 고속으로 검색하기 위한 구조,
  탐색은 빠르지만 미리 전처리를 거친 데이터 구조를 만들어야 하고 전처리 시간이 상당함

- 여기서 발생하는 전처리 시간을 어떻게 줄일 수 있을까 고민함

→ IS법(선형시간에 Suffix Array 정렬을 마치는 알고리즘)을 도입!

- Javascript를 이용해 알고리즘을 구현했지만 만족스럽지 않음
- 속도가 나도 전처리시간에는 시간이 걸리고
- 사용자가 북마크를 할 때마다 전처리를 수행하도록 하면 머신에 부하가 많이 걸림

→ 결국 Suffix Array를 버리고
Firefox 확장기능이 내부에 가지고 있는 SQLite에 SQL로 like에 의한 부분일치 검색(선형탐색)을 하도록 함

→ 오히려 속도가 개선된 것을 확인할 수 있었다

### 결론(배운점)

- 예측과 측정이 중요하다는 것
  - 때에 따라서는 명쾌하게 단순한 구현을 시도해보는 것이 좋다는 것
- 대규모 데이터를 상정한 경우에는 최적화도 중요하지만 데이터 건수가 작은 경우에는 최적화 의미가 없다.
- 데이터 건수가 '적다'는 것을 사람의 직감으로 추측하는 것은 좋지 않다.

## 써드파티 소스를 잘 활용하자

위의 사례에서는 필요한 알고리즘을 직접 구현해서 사용했지만
유면한 알고리즘들은 제 3자가 이용하기 쉽도록 이미 구현된 소스가 공개되어 있다.
이걸 잘 활용하면 개발 공수를 줄일 수 있다.

### 주의할 점

이미 구현된 소스가 있다고 해도
내부에 어떻게 구현되어 있는지 파악하지 않고 블랙박스인 채로 사용하는 것은 권장하지 않는다.

# 20. 하테나 다이어리의 키워드 링크

## 키워드 링크란?

블로그를 작성하면 일부 키워드에 링크가 자동으로 걸리고, 그 링크는 키워드를 설명하는 페이지로 이동시킨다.

- 링크 대상이 되는 키워드는 하테나 키워드에 사용자가 등록한 키워드들이다.

→ 즉 특정 키워드를 HTML anchor 태그로 치환하는 것!

- ex) 하테나 다이어리는 블로그다.
  → <a href=”..”>하테나 다이어리</a>는 <a href=”..”>블로그</a>다.

## 최초 구현 방법

사전 내에 포함된 모든 단어를 OR 조건으로 잇는 정규표현을 만들어 사용했다.

## 문제발생! - 키워드 사전의 대규모화

키워드가 적을 때에는 괜찮았지만,
키워드 수가 많아짐에 따라 정규 표현 처리에 시간이 걸리는 문제 발생!

- 정규표현을 컴파일하는 처리
  → 미리 정규표현을 만들어서 메모리나 디스크 상에 저장(캐싱)해둠으로써 회피할 수 있었다.
- 정규표현에서 패턴매칭하는 처리
  → 처음에는 키워드 링크가 완료된 본문 텍스트를 캐싱하는 식으로 회피할 수 있었다.
  - 하지만 새로 추가된 키워드를 키워드 링크에 반영시키기 위해서는 일정 시간에 캐시를 다시 구축할 필요가 있음.
  - 또 블로그 특성상 대부분을 차지하는 그다지 액세스가 없는 블로그에서는 캐시가 효과를 나타내기 어려움 등의 근본적인 해결에는 이르지 못함.


## 패턴매칭에 의한 키워드 링크의 문제점

키워드 어휘수가 10만개가 넘어가고 하테나 다이어리 액세스 수가 늘어나는 만큼 키워드 링크 처리 횟수도 늘어서 결국! 시스템이 혹사당하는 상태에 이르게 되었다.

키워드 링크에 계산 시간이 걸리는 원인은 정규표현의 알고리즘에 있다.

- 정규식에서는 (foo | bar| baz) 같은 패턴매칭을 앞에서부터 입력값을 살펴가면서
  매칭에 실패하면 다음 단어를 시도하고 또 실패하면 그 다음 단어를 시도하는 단순한 방법으로 처리한다.

→  즉 키워드 갯수에 비례하는 계산량이 소요된다.

## 정규표현 → Trie - 매칭 구현 변경

패턴매칭에 수반되는 계산량 문제를 해결하기 위해
정규표현 → Trie를 사용한 매칭 구현으로 변경을 했다.

Trie는 트리의 일종인 데이터 구조로,
탐색 대상 데이터의 공통 접두사를 모아서 트리구조를 이루는 것이 특징이다

![image.png](attachment:a1a8aa3c-9424-4eb7-b28f-55e7768be30c:image.png)

- 각 트리의 엣지에는 문자가 할당됨
- 공통 접두사를 중복 없이 하나로 정리함으로써 불필요한 것을 배제하고 있다.

### Trie 구조와 패턴 매칭

Trie 구조를 사전과 비교하면서 패턴매칭을 하면
정규표현 보다 계산량을 줄일 수 있다.

❗ 예를들어 hogefoo에서 (foo | bar| baz)  패턴을 찾는다면?

- Trie 사용
  - 트리 탐색시 h,o,g,e 없음 → f 있음 → 트리 순회해서 엣지에서 foo 찾음
  - 즉, hogefoo 단어 길이만큼만 계산하면 끝
- 정규표현 사용
  - h~o 까지 전부 (foo | bar| baz) 키워드가 매칭되는지 비교함
  - 키워드 수에 비례해서 계산량이 늘어나기 때문에
    키워드 사전이 커질 수록 계산량이 늘어난다.

## AC법 - Trie에 의한 매칭을 더욱 빠르게

실제로 하테나 다이어리를 개선했을 때는 AC법을 사용했다.
AC법을 사용하면  계산량이 사전 크기에 의존하지 않는다.

AC법은 Trie에서 패턴 매칭으로 매칭이 진행되다가
도중에 실패했을 때 되돌아오는 길의 엣지를 다시 Trie 에 추가한 데이터 구조를 사용하는 방법이다

### ❗예를 들어 “babcdes”를 입력한 경우

![image.png](attachment:9adcb524-30da-40a7-b452-d10f0ef0bcae:image.png)

- “bab”까지 탐색했다면, 선두노드인 0으로 돌아가는 것이 아니라
  바로 노드2로 갈 수 있다는 걸 알 수 있다면
  ”ab”를 바로 찾을 수 있다.
- 여기서 ‘6 다음은 바로 2’ 라고 길을 내는 전처리를 Trie에 대해 수행하는 것이
  AC법의 핵심이다.

## **Regexp:: List로의 치환**

AC 법을 직접 구현한 라이브러리를 사용하다 나중에는 Regexp::List라는 라이브러리로 치환했다.

- Regexp::List는 거대한 정규표현을 Trie에 최적화된 정규표현으로 변환한다.
- ex) “qw/foobar fooxar foozap fooza/” → “foo(?:[bx]ar|zap?)”

### 장점

- 공통 접두사/접미사가 정리되어 있어서
  패턴매칭 수행 시 OR로 모든 단어를 연결한 것보다 시행 횟수를 줄일 수 있다.
- 계산량이 줄어들고 정규표현으로 사용할 수 있다
  - 정규표현의 옵션 조합, 언어적인 기능을 조합할 수 있어 유연성이 있다.


## **키워드 링크 구현, 변이 및 고찰**

- 심플한 구현이 주효할 때도 있다.
  - 처음에 간단한 정규식을 사용할때, 개발 공수도 적고 유연성이 풍부했다.
  - 덕분에 테스트를 하거나 기능을 수정하기 쉬웠다.
- 데이터가 커질 때 문제가 여러가지 발생할 수 있고
  여기에는 본질적인 해결책이 필요하다.
  - 트래픽이 많아지면서 캐시 등을 도입해서 문제를 해결하려 했다.
    - 하지만 모든 케이스에 적합한 방법은 아니라서
      본질적으로 계산량을 줄여야 했음

# 21. 하테나 북마크의 기사 분류

하테나 북마크의 기사 분류 기능을 예로 특정 알고리즘으로 새로운 문제에 대응하는 예를 살펴본다.

## 기사 분류란?

하테나 북마크에 새로운 기사를 작성하면 북마크의 시스템은 해당 기사를 HTTP로 얻어서
본문 텍스트의 내용으로부터 분류해서 카테고리를 판정한다.

### 베이지안 필터에 의한 카테고리 판정

하테나는 `베이지안 필터` 를 사용해서 카테고리를 판정한다.

`베이지안 필터` 는

텍스트 문서 등을 입력으로 받아들이고 거기에 `나이브 베이즈`라는 알고리즘을 적용해서
확률적으로 해당 문서가 어느 카테고리에 속하는지를 판정하는 프로그램이다.

- 미지의 문서의 카테고리 판정을 수행함에 있어서 과거에 분류가 끝난 데이터의 통계정보로부터 판정을 수행한다는 점이 특징이다. ( 정해 데이터를 주고 프로그램 학습을 시킴)

## 베이지안 필터의 원리

베이지안 필터의 핵심은 `나이브 베이즈`라는 알고리즘이다.

### 나이브 베이즈에 근거한 카테고리 추정

나이브 베이즈에서 카테고리 추정은,
특정 문서 D가 주어졌을 때  이 문서가 어떤 카테고리 C에 속하는게 가장 그럴듯한가를 구하는 문제이다.

### 손쉬운 카테고리 추정 실현

나이브 베이즈에서는 정해 데이터가 주어지면 해당 정해 데이터가 사용된 횟수나 단어 출현횟수와 같이
간단한 수치만 저장해두면 나중에 확률만 계산하면 카테고리를 추정할 수 있다는 것이다.
그 밖의 데이터는 전부 파괴해도 상관 없다.

### ❗나만의 정리

→ 베이지안 필터를 통해 문서를 특징별로 분류할 수 있다.

- 메일 스팸 필터 등에서 사용됨

## 수비 자세,  공격 자세 - 기사 분류 구현으로 부터의 고찰

- 동일한 알고리즘이라도 대규모 데이터를 빠르게 정렬하거나 검색,압축하는 일은
  아무래도 발생하는 문제를 얼마나 잘 맞아들이는가라는 “수비”적인 자세에서 자주 사용하는 알고리즘이 아닐까?
- 반면, 기계학습이나 패턴인식 등은 적극적으로 대규모 데이터를 응용하고 그 결과에 따라 부가가치를 추가한다는 의미로 “공격”적인 자세로 사용되는 알고리즘이 아닐까 생각한다.

### 기존 방법 익혀두기

수비를 하던 공격을 하던 대규모 데이터에 대해 알고리즘 측면에서의 접근법을 익히려고 할 때,
기존 방법은 어느정도 자신의 지식으로 익혀두는 것이 중요하다.

- 키워드 링크에서 Trie를 응용할 수 있다는 발상은
  Trie의 특성을 모른다면 생각할 수 없었을 것이다.
- 베이지안 필터의 원리를 모르면 문서를 자동으로 분류한다는 발상은 할 수 없었을 것이다.